2013-10-23 22:42:10.927181
conclusions
{199: u'We have presented an unsupervised probabilistic answer type model.', 200: u'Our model uses contexts derived from the question and the candidate answer to calculate the appropriateness of a candidate answer.', 201: u'Statistics gathered from a large corpus of text are used in the calculation, and the model is constructed to exploit these statistics without being overly speci\ufb01c or overly general.', 202: u'The method presented here avoids the use of an explicit list of answer types.', 203: u'Explicit answer types can exhibit poor performance, especially for those questions not \ufb01tting one of the types.', 204: u'They must also be rede\ufb01ned when either the domain or corpus substantially changes.', 205: u'By avoiding their use, our answer typing method may be easier to adapt to different corpora and question answering domains (such as bioinformatics).', 206: u'In addition to operating as a stand-alone answer typing component, our system can be combined with other existing answer typing strategies, especially in situations in which a catch-all answer type is used.', 207: u'Our experimental results show that our probabilistic model outperforms the oracle and a system using automatic named entity recognition under such circumstances.', 208: u'The performance of our model is better than that of the semi-automatic system, which is a better indication of the expected performance of a comparable real-world answer typing system. '}
introduction
{5: u'Given a question, people are usually able to form an expectation about the type of the answer, even if they do not know the actual answer.', 6: u'An accurate expectation of the answer type makes it much easier to select the answer from a sentence that contains the query words.', 7: u'Consider the question \u201cWhat is the capital of Norway?\u201d We would expect the answer to be a city and could \ufb01lter out most of the words in the following sentence: The landed aristocracy was virtually crushed by Hakon V, who reigned from 1299 to 1319, and Oslo became the capital of Norway, replacing Bergen as the principal city of the kingdom.', 8: u'The goal of answer typing is to determine whether a word\u2019s semantic type is appropriate as an answer for a question.', 9: u'Many previous approaches to answer typing, e.g., (Ittycheriah et al., 2001; Li and Roth, 2002; Krishnan et al., 2005), employ a prede\ufb01ned set of answer types and use supervised learning or manually constructed rules to classify a question according to expected answer type.', 10: u'A disadvantage of this approach is that there will always be questions whose answers do not belong to any of the prede\ufb01ned types.', 11: u'Consider the question: \u201cWhat are tourist attractions in Reims?\u201d The answer may be many things: a church, a historic residence, a park, a famous intersection, a statue, etc.', 12: u'A common method to deal with this problem is to de\ufb01ne a catch-all class.', 13: u'This class, however, tends not to be as effective as other answer types.', 14: u'Another disadvantage of prede\ufb01ned answer types is with regard to granularity.', 15: u'If the types are too speci\ufb01c, they are more dif\ufb01cult to tag.', 16: u'If they are too general, too many candidates may be identi\ufb01ed as having the appropriate type.', 17: u'In contrast to previous approaches that use a supervised classi\ufb01er to categorize questions into a prede\ufb01ned set of types, we propose an unsupervised method to dynamically construct a probabilistic answer type model for each question.', 18: u'Such a model can be used to evaluate whether or not a word \ufb01ts into the question context.', 19: u'For example, given the question \u201cWhat are tourist attractions in Reims?\u201d, we would expect the appropriate answers to \ufb01t into the context \u201cX is a tourist attraction.\u201d From a corpus, we can \ufb01nd the words that appeared in this context, such as: A-Ama Temple, Aborigine, addition, Anak Krakatau, archipelago, area, baseball, Bletchley Park, brewery, cabaret, Cairo, Cape Town, capital, center, ...', 20: u'Using the frequency counts of these words in the context, we construct a probabilistic model to compute P(in(w, \u0393)|w), the probability for a word w to occur in a set of contexts \u0393, given an occurrence of w. The parameters in this model are obtained from a large, automatically parsed, unlabeled corpus.', 21: u'By asking whether a word would occur in a particular context extracted from a ques', 22: u'tion, we avoid explicitly specifying a list of possible answer types.', 23: u'This has the added bene\ufb01t of being easily adapted to different domains and corpora in which a list of explicit possible answer types may be dif\ufb01cult to enumerate and/or identify within the text.', 24: u'The remainder of this paper is organized as follows.', 25: u'Section 2 discusses the work related to answer typing.', 26: u'Section 3 discusses some of the key concepts employed by our probabilistic model, including word clusters and the contexts of a question and a word.', 27: u'Section 4 presents our probabilistic model for answer typing.', 28: u'Section 5 compares the performance of our model with that of an oracle and a semi-automatic system performing the same task.', 29: u'Finally, the concluding remarks in are made in Section 6. '}
abstract
{0: u'All questions are implicitly associated with an expected answer type.', 1: u'Unlike previous approaches that require a prede\ufb01ned set of question types, we present a method for dynamically constructing a probability-based answer type model for each different question.', 2: u'Our model evaluates the appropriateness of a potential answer by the probability that it \ufb01ts into the question contexts.', 3: u'Evaluation is performed against manual and semiautomatic methods using a \ufb01xed set of answer labels.', 4: u'Results show our approach to be superior for those questions classi\ufb01ed as having a miscellaneous answer type. '}
acknowledgments
{209: u'The authors would like to thank the anonymous reviewers for their helpful comments on improving the paper.', 210: u'The \ufb01rst author is supported by the Natural Sciences and Engineering Research Council of Canada, the Alberta Ingenuity Fund, and the Alberta Informatics Circle of Research Excellence. '}
related work
{30: u'Light et al.', 31: u'(2001) performed an analysis of the effect of multiple answer type occurrences in a sentence.', 32: u'When multiple words of the same type appear in a sentence, answer typing with \ufb01xed types must assign each the same score.', 33: u'Light et al.', 34: u'found that even with perfect answer sentence identi\ufb01cation, question typing, and semantic tagging, a system could only achieve 59% accuracy over the TREC-9 questions when using their set of 24 non-overlapping answer types.', 35: u'By computing the probability of an answer candidate occurring in the question contexts directly, we avoid having multiple candidates with the same level of appropriateness as answers.', 36: u'There have been a variety of approaches to determine the answer types, which are also known as Qtargets (Echihabi et al., 2003).', 37: u'Most previous approaches classify the answer type of a question as one of a set of prede\ufb01ned types.', 38: u'Many systems construct the classi\ufb01cation rules manually (Cui et al., 2004; Greenwood, 2004; Hermjakob, 2001).', 39: u'The rules are usually triggered by the presence of certain words in the question.', 40: u'For example, if a question contains \u201cauthor\u201d then the expected answer type is Person.', 41: u'The number of answer types as well as the number of rules can vary a great deal.', 42: u'For example, (Hermjakob, 2001) used 276 rules for 122 answer types.', 43: u'Greenwood (2004), on the other hand, used 46 answer types with unspeci\ufb01ed number of rules.', 44: u'The classi\ufb01cation rules can also be acquired with supervised learning.', 45: u'Ittycheriah, et al.', 46: u'(2001) describe a maximum entropy based question classi\ufb01cation scheme to classify each question as having one of the MUC answer types.', 47: u'In a similar experiment, Li & Roth (2002) train a question classi\ufb01er based on a modi\ufb01ed version of SNoW using a richer set of answer types than Ittycheriah et al.', 48: u'The LCC system (Harabagiu et al., 2003) combines \ufb01xed types with a novel loop-back strategy.', 49: u'In the event that a question cannot be classi\ufb01ed as one of the \ufb01xed entity types or semantic concepts derived from WordNet (Fellbaum, 1998), the answer type model backs off to a logic prover that uses axioms derived form WordNet, along with logic rules, to justify phrases as answers.', 50: u'Thus, the LCC system is able to avoid the use of a miscellaneous type that often exhibits poor performance.', 51: u'However, the logic prover must have suf\ufb01cient evidence to link the question to the answer, and general knowledge must be encoded as axioms into the system.', 52: u'In contrast, our answer type model derives all of its information automatically from unannotated text.', 53: u'Answer types are often used as \ufb01lters.', 54: u'It was noted in (Radev et al., 2002) that a wrong guess about the answer type reduces the chance for the system to answer the question correctly by as much as 17 times.', 55: u'The approach presented here is less brittle.', 56: u'Even if the correct candidate does not have the highest likelihood according to the model, it may still be selected when the answer extraction module takes into account other factors such as the proximity to the matched keywords.', 57: u'Furthermore, a probabilistic model makes it easier to integrate the answer type scores with scores computed by other components in a question answering system in a principled fashion. '}
evaluation
{149: u'We evaluate our answer typing system by using it to \ufb01lter the contents of documents retrieved by the information retrieval portion of a question answering system.', 150: u'Each answer candidate in the set of documents is scored by the answer typing system and the list is sorted in descending order of score.', 151: u'We treat the system as a \ufb01lter and observe the proportion of candidates that must be accepted by the \ufb01lter so that at least one correct answer is accepted.', 152: u'A model that allows a low percentage of candidates to pass while still allowing at least one correct answer through is favorable to a model in which a high number of candidates must pass.', 153: u'This represents an intrinsic rather than extrinsic evaluation (Moll\xb4a and Hutchinson, 2003) that we believe illustrates the usefulness of our model.', 154: u'The evaluation data consist of 154 questions from the TREC-2003 QA Track (Voorhees, 2003) satisfying the following criteria, along with the top ', 155: u'We compare the performance of our probabilistic model with that of two other systems.', 156: u'Both comparison systems make use of a small, prede\ufb01ned set of manually-assigned MUC7 named-entity types (location, person, organization, cardinal, percent, date, time, duration, measure, money) augmented with thing-name (proper ', 157: u'trec.nist.gov/data/qa/2003 qadata/03QA.tasks/t12.pats.txt names of inanimate objects) and miscellaneous (a catch-all answer type of all other candidates).', 158: u'Some examples of thing-name are Guinness Book of World Records, Thriller, Mars Path\ufb01nder, and Grey Cup.', 159: u'Examples of miscellaneous answers are copper, oil, red, and iris.', 160: u'The differences in the comparison systems is with respect to how entity types are assigned to the words in the candidate documents.', 161: u'We make use of the ANNIE (Maynard et al., 2002) named entity recognition system, along with a manual assigned \u201coracle\u201d strategy, to assign types to candidate answers.', 162: u'In each case, the score for a candidate is either 1 if it is tagged as the same type as the question or 0 otherwise.', 163: u'With this scoring scheme producing a sorted list we can compute the probability of the \ufb01rst correct answer appearing at rank R = k as follows: ', 164: u'where t is the number of unique candidate answers that are of the appropriate type and c is the number of unique candidate answers that are correct.', 165: u'Using the probabilities in equation (15), we compute the expected rank, E(R), of the \ufb01rst correct answer of a given question in the system as: ', 166: u'Answer candidates are the set of ANNIEidenti\ufb01ed tokens with stop words and punctuation removed.', 167: u'This yields between 900 and 8000 candidates for each question, depending on the top 10 documents returned by PRISE.', 168: u'The oracle system represents an upper bound on using the prede\ufb01ned set of answer types.', 169: u'The ANNIE system represents a more realistic expectation of performance.', 170: u'The median percentage of candidates that are accepted by a \ufb01lter over the questions of our evaluation data provides one measure of performance and is preferred to the average because of the effect of large values on the average.', 171: u'In QA, a system accepting 60% of the candidates is not significantly better or worse than one accepting 100%, ', 172: u'but the effect on average is quite high.', 173: u'Another measure is to observe the number of questions with at least one correct answer in the top N% for various values of N. By examining the number of correct answers found in the top N% we can better understand what an effective cutoff would be.', 174: u'The overall results of our comparison can be found in Table 2.', 175: u'We have added the results of a system that scores candidates based on their frequency within the document as a comparison with a simple, yet effective, strategy.', 176: u'The second column is the median percentage of where the highest scored correct answer appears in the sorted candidate list.', 177: u'Low percentage values mean the answer is usually found high in the sorted list.', 178: u'The remaining columns list the number of questions that have a correct answer somewhere in the top N% of their sorted lists.', 179: u'This is meant to show the effects of imposing a strict cutoff prior to running the answer type model.', 180: u'The oracle system performs best, as it bene\ufb01ts from both manual question classi\ufb01cation and manual entity tagging.', 181: u'If entity assignment is performed by an automatic system (as it is for ANNIE), the performance drops noticeably.', 182: u'Our probabilistic model performs better than ANNIE and achieves approximately 2/3 of the performance of the oracle system.', 183: u'Table 2 also shows that the use of candidate contexts increases the performance of our answer type model.', 184: u'Table 3 shows the performance of the oracle system, our model, and the ANNIE system broken down by manually-assigned answer types.', 185: u'Due to insuf\ufb01cient numbers of questions, the cardinal, percent, time, duration, measure, and money types are combined into an \u201cOther\u201d category.', 186: u'When compared with the oracle system, our model performs worse overall for questions of all types except for those seeking miscellaneous answers.', 187: u'For miscellaneous questions, the oracle identi\ufb01es all tokens that do not belong to one of the other known categories as possible answers.', 188: u'For all questions of non-miscellaneous type, only a small subset of the candidates are marked appropriate.', 189: u'In particular, our model performs worse than the oracle for questions seeking persons and thingnames.', 190: u'Person questions often seek rare person names, which occur in few contexts and are dif\ufb01cult to reliably cluster.', 191: u'Thing-name questions are easy for a human to identify but dif\ufb01cult for automatic system to identify.', 192: u'Thing-names are a diverse category and are not strongly associated with any identifying contexts.', 193: u'Our model outperforms the ANNIE system in general, and for questions seeking organizations, thing-names, and miscellaneous targets in particular.', 194: u'ANNIE may have low coverage on organization names, resulting in reduced performance.', 195: u'Like the oracle, ANNIE treats all candidates not assigned one of the categories as appropriate for miscellaneous questions.', 196: u'Because ANNIE cannot identify thing-names, they are treated as miscellaneous.', 197: u'ANNIE shows low performance on thingnames because words incorrectly assigned types are sorted to the bottom of the list for miscellaneous and thing-name questions.', 198: u'If a correct answer is incorrectly assigned a type it will be sorted near the bottom, resulting in a poor score. '}
method
{58: u'Before introducing our model, we \ufb01rst describe the resources used in the model. ', 59: 'Natural language data is extremely sparse.', 60: 'Word clusters are a way of coping with data sparseness by abstracting a given word to a class of related words.', 61: 'Clusters, as used by our probabilistic answer typing system, play a role similar to that of named entity types.', 62: 'Many methods exist for clustering, e.g., (Brown et al., 1990; Cutting et al., 1992; Pereira et al., 1993; Karypis et al., 1999).', 63: 'We used the Clustering By Committee (CBC) ', 64: u'suite software, network, wireless, ... rooms, bathrooms, restrooms, ... meeting room, conference room, ... ghost rabbit, squirrel, duck, elephant, frog, ... goblins, ghosts, vampires, ghouls, ... punk, reggae, folk, pop, hip-pop, ... huge, larger, vast, signi\ufb01cant, ... coming-of-age, true-life, ... clouds, cloud, fog, haze, mist, ... algorithm (Pantel and Lin, 2002) on a 10 GB English text corpus to obtain 3607 clusters.', 65: u'The following is an example cluster generated by CBC: tension, anger, anxiety, tensions, frustration, resentment, uncertainty, confusion, con\ufb02ict, discontent, insecurity, controversy, unease, bitterness, dispute, disagreement, nervousness, sadness, despair, animosity, hostility, outrage, discord, pessimism, anguish, ...', 66: u'In the clustering generated by CBC, a word may belong to multiple clusters.', 67: u'The clusters to which a word belongs often represent the senses of the word.', 68: u'Table 1 shows two example words and their clusters. ', 69: u'The context in which a word appears often imposes constraints on the semantic type of the word.', 70: u'This basic idea has been exploited by many proposals for distributional similarity and clustering, e.g., (Church and Hanks, 1989; Lin, 1998; Pereira et al., 1993).', 71: u'Similar to Lin and Pantel (2001), we de\ufb01ne the contexts of a word to be the undirected paths in dependency trees involving that word at either the beginning or the end.', 72: u'The following diagram shows an example dependency tree: Which city hosted the 1988 Winter Olympics? ', 73: u'The links in the tree represent dependency relationships.', 74: u'The direction of a link is from the head to the modi\ufb01er in the relationship.', 75: u'Labels associated with the links represent types of relations.', 76: u'In a context, the word itself is replaced with a variable X.', 77: u'We say a word is the \ufb01ller of a context if it replaces X.', 78: u'For example, the contexts for the word \u201cOlympics\u201d in the above sentence include the following paths: ', 79: u'In these paths, words are reduced to their root forms and proper names are reduced to their entity tags (we used MUC7 named entity tags).', 80: u'Paths allow us to balance the speci\ufb01city of contexts and the sparseness of data.', 81: u'Longer paths typically impose stricter constraints on the slot \ufb01llers.', 82: u'However, they tend to have fewer occurrences, making them more prone to errors arising from data sparseness.', 83: u'We have restricted the path length to two (involving at most three words) and require the two ends of the path to be nouns.', 84: u'We parsed the AQUAINT corpus (3GB) with Minipar (Lin, 2001) and collected the frequency counts of words appearing in various contexts.', 85: u'Parsing and database construction is performed off-line as the database is identical for all questions.', 86: u'We extracted 527,768 contexts that appeared at least 25 times in the corpus.', 87: u'An example context and its \ufb01llers are shown in Figure 1. ', 88: u'To build a probabilistic model for answer typing, we extract a set of contexts, called question contexts, from a question.', 89: u'An answer is expected to be a plausible \ufb01ller of the question contexts.', 90: u'Question contexts are extracted from a question with two rules.', 91: u'First, if the wh-word in a question has a trace in the parse tree, the question contexts are the contexts of the trace.', 92: u'For example, the ', 93: u'question \u201cWhat do most tourists visit in Reims?\u201d is parsed as: Whati do most tourists visit ei in Reims? ', 94: 'The symbol ei is the trace of whati.', 95: 'Minipar generates the trace to indicate that the word what is the object of visit in the deep structure of the sentence.', 96: 'The following question contexts are extracted from the above question: ', 97: u'The second rule deals with situations where the wh-word is a determiner, as in the question \u201cWhich city hosted the 1988 Winter Olympics?\u201d (the parse tree for which is shown in section 3.2).', 98: u'In such cases, the question contexts consist of a single context involving the noun that is modi\ufb01ed by the determiner.', 99: u'The context for the above sentence is X city subj , corresponding to the sentence \u201cX is a city.\u201d This context is used because the question explicitly states that the desired answer is a city.', 100: u'The context overrides the other contexts because the question explicitly states the desired answer type.', 101: u'Experimental results have shown that using this context in conjunction with other contexts extracted from the question produces lower performance than using this context alone.', 102: u'In the event that a context extracted from a question is not found in the database, we shorten the context in one of two ways.', 103: u'We start by replacing the word at the end of the path with a wildcard that matches any word.', 104: u'If this fails to yield entries in the context database, we shorten the context to length one and replace the end word with automatically determined similar words instead of a wildcard. ', 105: u'Candidate contexts are very similar in form to question contexts, save for one important difference.', 106: u'Candidate contexts are extracted from the parse trees of the answer candidates rather than the question.', 107: u'In natural language, some words may be polysemous.', 108: u'For example, Washington may refer to a person, a city, or a state.', 109: u'The occurrences of Washington in \u201cWashington\u2019s descendants\u201d and \u201csuburban Washington\u201d should not be given the same score when the question is seeking a location.', 110: u'Given that the sense of a word is largely determined by its local context (Choueka and Lusignan, 1985), candidate contexts allow the model to take into account the candidate answers\u2019 senses implicitly. ', 111: u'The goal of an answer typing model is to evaluate the appropriateness of a candidate word as an answer to the question.', 112: u'If we assume that a set of answer candidates is provided to our model by some means (e.g., words comprising documents extracted by an information retrieval engine), we wish to compute the value P(in(w, \u0393Q)|w).', 113: u'That is, the appropriateness of a candidate answer w is proportional to the probability that it will occur in the question contexts \u0393Q extracted from the question.', 114: u'To mitigate data sparseness, we can introduce a hidden variable C that represents the clusters to which the candidate answer may belong.', 115: u'As a candidate may belong to multiple clusters, we obtain: ', 116: 'Given that a word appears, we assume that it has the same probability to appear in a context as all other words in the same cluster.', 117: 'Therefore: ', 118: 'We can now rewrite the equation in (2) as: ', 119: u'This equation splits our model into two parts: one models which clusters a word belongs to and the other models how appropriate a cluster is to the question contexts.', 120: u'When \u0393Q consists of multiple contexts, we make the na\xa8\u0131ve Bayes assumption that each individual context \u03b3Q \u2208 \u0393Q is independent of all other contexts given the cluster C. ', 121: u'Equation (5) needs the parameters P(C|w) and P(in(C, \u03b3Q)|C), neither of which are directly available from the context-\ufb01ller database.', 122: u'We will discuss the estimation of these parameters in Section 4.2. ', 123: u'The previous model assigns the same likelihood to every instance of a given word.', 124: u'As we noted in section 3.2.2, a word may be polysemous.', 125: u'To take into account a word\u2019s context, we can instead compute P(in(w, \u0393Q)|w, in(w, \u0393w)), where \u0393w is the set of contexts for the candidate word w in a retrieved passage.', 126: u'By introducing word clusters as intermediate variables as before and making a similar assumption as in equation (3), we obtain: ', 127: 'Like equation (4), equation (7) partitions the model into two parts.', 128: 'Unlike P(C|w) in equation (4), the probability of the cluster is now based on the particular occurrence of the word in the candidate contexts.', 129: 'It can be estimated by: ', 130: u'Our probabilistic model requires the parameters P(C|w), P(C|w, in(w, \u03b3)), and P(in(C, \u03b3)|C), where w is a word, C is a cluster that w belongs to, and \u03b3 is a question or candidate context.', 131: u'This section explains how these parameters are estimated without using labeled data.', 132: u'The context-\ufb01ller database described in Section 3.2 provides the joint and marginal frequency counts of contexts and words (|in(\u03b3, w)|, |in(\u2217, \u03b3) |and |in(w, \u2217)|).', 133: u'These counts allow us to compute the probabilities P(in(w, \u03b3)), P(in(w, \u2217)), and P(in(\u2217, \u03b3)).', 134: u'We can also compute P(in(w, \u03b3)|w), which is smoothed with addone smoothing (see equation (11) in Figure 2).', 135: u'The estimation of P(C|w) presents a challenge.', 136: u'We have no corpus from which we can directly measure P(C|w) because word instances are not labeled with their clusters. ', 137: u'We use the average weighted \u201cguesses\u201d of the top similar words of w to compute P(C|w) (see equation 13).', 138: u'The intuition is that if w and w are similar words, P(C|w ) and P(C|w) tend to have similar values.', 139: u'Since we do not know P(C|w ) either, we substitute it with uniform distribution Pu(C|w ) as in equation (12) of Figure 2.', 140: u'Although Pu(C|w ) is a very crude guess, the weighted average of a set of such guesses can often be quite accurate.', 141: u'The similarities between words are obtained as a byproduct of the CBC algorithm.', 142: u'For each word, we use S(w) to denote the top-n most similar words (n=50 in our experiments) and sim(w, w ) to denote the similarity between words w and w .', 143: u'The following is a sample similar word list for the ', 144: u'plaint 0.29, lawsuits 0.27, jacket 0.25, countersuit 0.24, counterclaim 0.24, pants 0.24, trousers 0.22, shirt 0.21, slacks 0.21, case 0.21, pantsuit 0.21, shirts 0.20, sweater 0.20, coat 0.20, ...} The estimation for P(C|w, in(w, \u03b3w)) is similar to that of P(C|w) except that instead of all w \u2208 S(w), we instead use {w |w \u2208 S(w) \u2227 in(w , \u03b3w)}.', 145: u'By only looking at a particular context \u03b3w, we may obtain a different distribution over C than P(C|w) speci\ufb01es.', 146: u'In the event that the data are too sparse to estimate P(C|w, in(w, \u03b3w)), we fall back to using P(C|w).', 147: u'P(in(C, \u03b3)|C) is computed in (14) by assuming each instance of w contains a fractional instance of C and the fractional count is P(C|w).', 148: u'Again, add-one smoothing is used. '}
2013-10-24 01:33:43.777905
2013-10-24 01:34:30.179559
2013-10-24 01:34:54.810501
2013-10-24 01:41:28.585659
2013-10-24 01:43:43.535755
conclusions
We have presented an unsupervised probabilistic answer type model.
Our model uses contexts derived from the question and the candidate answer to calculate the appropriateness of a candidate answer.
Statistics gathered from a large corpus of text are used in the calculation, and the model is constructed to exploit these statistics without being overly speciﬁc or overly general.
The method presented here avoids the use of an explicit list of answer types.
Explicit answer types can exhibit poor performance, especially for those questions not ﬁtting one of the types.
They must also be redeﬁned when either the domain or corpus substantially changes.
By avoiding their use, our answer typing method may be easier to adapt to different corpora and question answering domains (such as bioinformatics).
In addition to operating as a stand-alone answer typing component, our system can be combined with other existing answer typing strategies, especially in situations in which a catch-all answer type is used.
Our experimental results show that our probabilistic model outperforms the oracle and a system using automatic named entity recognition under such circumstances.
The performance of our model is better than that of the semi-automatic system, which is a better indication of the expected performance of a comparable real-world answer typing system. 
introduction
Given a question, people are usually able to form an expectation about the type of the answer, even if they do not know the actual answer.
An accurate expectation of the answer type makes it much easier to select the answer from a sentence that contains the query words.
Consider the question “What is the capital of Norway?” We would expect the answer to be a city and could ﬁlter out most of the words in the following sentence: The landed aristocracy was virtually crushed by Hakon V, who reigned from 1299 to 1319, and Oslo became the capital of Norway, replacing Bergen as the principal city of the kingdom.
The goal of answer typing is to determine whether a word’s semantic type is appropriate as an answer for a question.
Many previous approaches to answer typing, e.g., (Ittycheriah et al., 2001; Li and Roth, 2002; Krishnan et al., 2005), employ a predeﬁned set of answer types and use supervised learning or manually constructed rules to classify a question according to expected answer type.
A disadvantage of this approach is that there will always be questions whose answers do not belong to any of the predeﬁned types.
Consider the question: “What are tourist attractions in Reims?” The answer may be many things: a church, a historic residence, a park, a famous intersection, a statue, etc.
A common method to deal with this problem is to deﬁne a catch-all class.
This class, however, tends not to be as effective as other answer types.
Another disadvantage of predeﬁned answer types is with regard to granularity.
If the types are too speciﬁc, they are more difﬁcult to tag.
If they are too general, too many candidates may be identiﬁed as having the appropriate type.
In contrast to previous approaches that use a supervised classiﬁer to categorize questions into a predeﬁned set of types, we propose an unsupervised method to dynamically construct a probabilistic answer type model for each question.
Such a model can be used to evaluate whether or not a word ﬁts into the question context.
For example, given the question “What are tourist attractions in Reims?”, we would expect the appropriate answers to ﬁt into the context “X is a tourist attraction.” From a corpus, we can ﬁnd the words that appeared in this context, such as: A-Ama Temple, Aborigine, addition, Anak Krakatau, archipelago, area, baseball, Bletchley Park, brewery, cabaret, Cairo, Cape Town, capital, center, ...
Using the frequency counts of these words in the context, we construct a probabilistic model to compute P(in(w, Γ)|w), the probability for a word w to occur in a set of contexts Γ, given an occurrence of w. The parameters in this model are obtained from a large, automatically parsed, unlabeled corpus.
By asking whether a word would occur in a particular context extracted from a ques
tion, we avoid explicitly specifying a list of possible answer types.
This has the added beneﬁt of being easily adapted to different domains and corpora in which a list of explicit possible answer types may be difﬁcult to enumerate and/or identify within the text.
The remainder of this paper is organized as follows.
Section 2 discusses the work related to answer typing.
Section 3 discusses some of the key concepts employed by our probabilistic model, including word clusters and the contexts of a question and a word.
Section 4 presents our probabilistic model for answer typing.
Section 5 compares the performance of our model with that of an oracle and a semi-automatic system performing the same task.
Finally, the concluding remarks in are made in Section 6. 
abstract
All questions are implicitly associated with an expected answer type.
Unlike previous approaches that require a predeﬁned set of question types, we present a method for dynamically constructing a probability-based answer type model for each different question.
Our model evaluates the appropriateness of a potential answer by the probability that it ﬁts into the question contexts.
Evaluation is performed against manual and semiautomatic methods using a ﬁxed set of answer labels.
Results show our approach to be superior for those questions classiﬁed as having a miscellaneous answer type. 
acknowledgments
The authors would like to thank the anonymous reviewers for their helpful comments on improving the paper.
The ﬁrst author is supported by the Natural Sciences and Engineering Research Council of Canada, the Alberta Ingenuity Fund, and the Alberta Informatics Circle of Research Excellence. 
related work
Light et al.
(2001) performed an analysis of the effect of multiple answer type occurrences in a sentence.
When multiple words of the same type appear in a sentence, answer typing with ﬁxed types must assign each the same score.
Light et al.
found that even with perfect answer sentence identiﬁcation, question typing, and semantic tagging, a system could only achieve 59% accuracy over the TREC-9 questions when using their set of 24 non-overlapping answer types.
By computing the probability of an answer candidate occurring in the question contexts directly, we avoid having multiple candidates with the same level of appropriateness as answers.
There have been a variety of approaches to determine the answer types, which are also known as Qtargets (Echihabi et al., 2003).
Most previous approaches classify the answer type of a question as one of a set of predeﬁned types.
Many systems construct the classiﬁcation rules manually (Cui et al., 2004; Greenwood, 2004; Hermjakob, 2001).
The rules are usually triggered by the presence of certain words in the question.
For example, if a question contains “author” then the expected answer type is Person.
The number of answer types as well as the number of rules can vary a great deal.
For example, (Hermjakob, 2001) used 276 rules for 122 answer types.
Greenwood (2004), on the other hand, used 46 answer types with unspeciﬁed number of rules.
The classiﬁcation rules can also be acquired with supervised learning.
Ittycheriah, et al.
(2001) describe a maximum entropy based question classiﬁcation scheme to classify each question as having one of the MUC answer types.
In a similar experiment, Li & Roth (2002) train a question classiﬁer based on a modiﬁed version of SNoW using a richer set of answer types than Ittycheriah et al.
The LCC system (Harabagiu et al., 2003) combines ﬁxed types with a novel loop-back strategy.
In the event that a question cannot be classiﬁed as one of the ﬁxed entity types or semantic concepts derived from WordNet (Fellbaum, 1998), the answer type model backs off to a logic prover that uses axioms derived form WordNet, along with logic rules, to justify phrases as answers.
Thus, the LCC system is able to avoid the use of a miscellaneous type that often exhibits poor performance.
However, the logic prover must have sufﬁcient evidence to link the question to the answer, and general knowledge must be encoded as axioms into the system.
In contrast, our answer type model derives all of its information automatically from unannotated text.
Answer types are often used as ﬁlters.
It was noted in (Radev et al., 2002) that a wrong guess about the answer type reduces the chance for the system to answer the question correctly by as much as 17 times.
The approach presented here is less brittle.
Even if the correct candidate does not have the highest likelihood according to the model, it may still be selected when the answer extraction module takes into account other factors such as the proximity to the matched keywords.
Furthermore, a probabilistic model makes it easier to integrate the answer type scores with scores computed by other components in a question answering system in a principled fashion. 
evaluation
We evaluate our answer typing system by using it to ﬁlter the contents of documents retrieved by the information retrieval portion of a question answering system.
Each answer candidate in the set of documents is scored by the answer typing system and the list is sorted in descending order of score.
We treat the system as a ﬁlter and observe the proportion of candidates that must be accepted by the ﬁlter so that at least one correct answer is accepted.
A model that allows a low percentage of candidates to pass while still allowing at least one correct answer through is favorable to a model in which a high number of candidates must pass.
This represents an intrinsic rather than extrinsic evaluation (Moll´a and Hutchinson, 2003) that we believe illustrates the usefulness of our model.
The evaluation data consist of 154 questions from the TREC-2003 QA Track (Voorhees, 2003) satisfying the following criteria, along with the top 
We compare the performance of our probabilistic model with that of two other systems.
Both comparison systems make use of a small, predeﬁned set of manually-assigned MUC7 named-entity types (location, person, organization, cardinal, percent, date, time, duration, measure, money) augmented with thing-name (proper 
trec.nist.gov/data/qa/2003 qadata/03QA.tasks/t12.pats.txt names of inanimate objects) and miscellaneous (a catch-all answer type of all other candidates).
Some examples of thing-name are Guinness Book of World Records, Thriller, Mars Pathﬁnder, and Grey Cup.
Examples of miscellaneous answers are copper, oil, red, and iris.
The differences in the comparison systems is with respect to how entity types are assigned to the words in the candidate documents.
We make use of the ANNIE (Maynard et al., 2002) named entity recognition system, along with a manual assigned “oracle” strategy, to assign types to candidate answers.
In each case, the score for a candidate is either 1 if it is tagged as the same type as the question or 0 otherwise.
With this scoring scheme producing a sorted list we can compute the probability of the ﬁrst correct answer appearing at rank R = k as follows: 
where t is the number of unique candidate answers that are of the appropriate type and c is the number of unique candidate answers that are correct.
Using the probabilities in equation (15), we compute the expected rank, E(R), of the ﬁrst correct answer of a given question in the system as: 
Answer candidates are the set of ANNIEidentiﬁed tokens with stop words and punctuation removed.
This yields between 900 and 8000 candidates for each question, depending on the top 10 documents returned by PRISE.
The oracle system represents an upper bound on using the predeﬁned set of answer types.
The ANNIE system represents a more realistic expectation of performance.
The median percentage of candidates that are accepted by a ﬁlter over the questions of our evaluation data provides one measure of performance and is preferred to the average because of the effect of large values on the average.
In QA, a system accepting 60% of the candidates is not significantly better or worse than one accepting 100%, 
but the effect on average is quite high.
Another measure is to observe the number of questions with at least one correct answer in the top N% for various values of N. By examining the number of correct answers found in the top N% we can better understand what an effective cutoff would be.
The overall results of our comparison can be found in Table 2.
We have added the results of a system that scores candidates based on their frequency within the document as a comparison with a simple, yet effective, strategy.
The second column is the median percentage of where the highest scored correct answer appears in the sorted candidate list.
Low percentage values mean the answer is usually found high in the sorted list.
The remaining columns list the number of questions that have a correct answer somewhere in the top N% of their sorted lists.
This is meant to show the effects of imposing a strict cutoff prior to running the answer type model.
The oracle system performs best, as it beneﬁts from both manual question classiﬁcation and manual entity tagging.
If entity assignment is performed by an automatic system (as it is for ANNIE), the performance drops noticeably.
Our probabilistic model performs better than ANNIE and achieves approximately 2/3 of the performance of the oracle system.
Table 2 also shows that the use of candidate contexts increases the performance of our answer type model.
Table 3 shows the performance of the oracle system, our model, and the ANNIE system broken down by manually-assigned answer types.
Due to insufﬁcient numbers of questions, the cardinal, percent, time, duration, measure, and money types are combined into an “Other” category.
When compared with the oracle system, our model performs worse overall for questions of all types except for those seeking miscellaneous answers.
For miscellaneous questions, the oracle identiﬁes all tokens that do not belong to one of the other known categories as possible answers.
For all questions of non-miscellaneous type, only a small subset of the candidates are marked appropriate.
In particular, our model performs worse than the oracle for questions seeking persons and thingnames.
Person questions often seek rare person names, which occur in few contexts and are difﬁcult to reliably cluster.
Thing-name questions are easy for a human to identify but difﬁcult for automatic system to identify.
Thing-names are a diverse category and are not strongly associated with any identifying contexts.
Our model outperforms the ANNIE system in general, and for questions seeking organizations, thing-names, and miscellaneous targets in particular.
ANNIE may have low coverage on organization names, resulting in reduced performance.
Like the oracle, ANNIE treats all candidates not assigned one of the categories as appropriate for miscellaneous questions.
Because ANNIE cannot identify thing-names, they are treated as miscellaneous.
ANNIE shows low performance on thingnames because words incorrectly assigned types are sorted to the bottom of the list for miscellaneous and thing-name questions.
If a correct answer is incorrectly assigned a type it will be sorted near the bottom, resulting in a poor score. 
method
Before introducing our model, we ﬁrst describe the resources used in the model. 
Natural language data is extremely sparse.
Word clusters are a way of coping with data sparseness by abstracting a given word to a class of related words.
Clusters, as used by our probabilistic answer typing system, play a role similar to that of named entity types.
Many methods exist for clustering, e.g., (Brown et al., 1990; Cutting et al., 1992; Pereira et al., 1993; Karypis et al., 1999).
We used the Clustering By Committee (CBC) 
suite software, network, wireless, ... rooms, bathrooms, restrooms, ... meeting room, conference room, ... ghost rabbit, squirrel, duck, elephant, frog, ... goblins, ghosts, vampires, ghouls, ... punk, reggae, folk, pop, hip-pop, ... huge, larger, vast, signiﬁcant, ... coming-of-age, true-life, ... clouds, cloud, fog, haze, mist, ... algorithm (Pantel and Lin, 2002) on a 10 GB English text corpus to obtain 3607 clusters.
The following is an example cluster generated by CBC: tension, anger, anxiety, tensions, frustration, resentment, uncertainty, confusion, conﬂict, discontent, insecurity, controversy, unease, bitterness, dispute, disagreement, nervousness, sadness, despair, animosity, hostility, outrage, discord, pessimism, anguish, ...
In the clustering generated by CBC, a word may belong to multiple clusters.
The clusters to which a word belongs often represent the senses of the word.
Table 1 shows two example words and their clusters. 
The context in which a word appears often imposes constraints on the semantic type of the word.
This basic idea has been exploited by many proposals for distributional similarity and clustering, e.g., (Church and Hanks, 1989; Lin, 1998; Pereira et al., 1993).
Similar to Lin and Pantel (2001), we deﬁne the contexts of a word to be the undirected paths in dependency trees involving that word at either the beginning or the end.
The following diagram shows an example dependency tree: Which city hosted the 1988 Winter Olympics? 
The links in the tree represent dependency relationships.
The direction of a link is from the head to the modiﬁer in the relationship.
Labels associated with the links represent types of relations.
In a context, the word itself is replaced with a variable X.
We say a word is the ﬁller of a context if it replaces X.
For example, the contexts for the word “Olympics” in the above sentence include the following paths: 
In these paths, words are reduced to their root forms and proper names are reduced to their entity tags (we used MUC7 named entity tags).
Paths allow us to balance the speciﬁcity of contexts and the sparseness of data.
Longer paths typically impose stricter constraints on the slot ﬁllers.
However, they tend to have fewer occurrences, making them more prone to errors arising from data sparseness.
We have restricted the path length to two (involving at most three words) and require the two ends of the path to be nouns.
We parsed the AQUAINT corpus (3GB) with Minipar (Lin, 2001) and collected the frequency counts of words appearing in various contexts.
Parsing and database construction is performed off-line as the database is identical for all questions.
We extracted 527,768 contexts that appeared at least 25 times in the corpus.
An example context and its ﬁllers are shown in Figure 1. 
To build a probabilistic model for answer typing, we extract a set of contexts, called question contexts, from a question.
An answer is expected to be a plausible ﬁller of the question contexts.
Question contexts are extracted from a question with two rules.
First, if the wh-word in a question has a trace in the parse tree, the question contexts are the contexts of the trace.
For example, the 
question “What do most tourists visit in Reims?” is parsed as: Whati do most tourists visit ei in Reims? 
The symbol ei is the trace of whati.
Minipar generates the trace to indicate that the word what is the object of visit in the deep structure of the sentence.
The following question contexts are extracted from the above question: 
The second rule deals with situations where the wh-word is a determiner, as in the question “Which city hosted the 1988 Winter Olympics?” (the parse tree for which is shown in section 3.2).
In such cases, the question contexts consist of a single context involving the noun that is modiﬁed by the determiner.
The context for the above sentence is X city subj , corresponding to the sentence “X is a city.” This context is used because the question explicitly states that the desired answer is a city.
The context overrides the other contexts because the question explicitly states the desired answer type.
Experimental results have shown that using this context in conjunction with other contexts extracted from the question produces lower performance than using this context alone.
In the event that a context extracted from a question is not found in the database, we shorten the context in one of two ways.
We start by replacing the word at the end of the path with a wildcard that matches any word.
If this fails to yield entries in the context database, we shorten the context to length one and replace the end word with automatically determined similar words instead of a wildcard. 
Candidate contexts are very similar in form to question contexts, save for one important difference.
Candidate contexts are extracted from the parse trees of the answer candidates rather than the question.
In natural language, some words may be polysemous.
For example, Washington may refer to a person, a city, or a state.
The occurrences of Washington in “Washington’s descendants” and “suburban Washington” should not be given the same score when the question is seeking a location.
Given that the sense of a word is largely determined by its local context (Choueka and Lusignan, 1985), candidate contexts allow the model to take into account the candidate answers’ senses implicitly. 
The goal of an answer typing model is to evaluate the appropriateness of a candidate word as an answer to the question.
If we assume that a set of answer candidates is provided to our model by some means (e.g., words comprising documents extracted by an information retrieval engine), we wish to compute the value P(in(w, ΓQ)|w).
That is, the appropriateness of a candidate answer w is proportional to the probability that it will occur in the question contexts ΓQ extracted from the question.
To mitigate data sparseness, we can introduce a hidden variable C that represents the clusters to which the candidate answer may belong.
As a candidate may belong to multiple clusters, we obtain: 
Given that a word appears, we assume that it has the same probability to appear in a context as all other words in the same cluster.
Therefore: 
We can now rewrite the equation in (2) as: 
This equation splits our model into two parts: one models which clusters a word belongs to and the other models how appropriate a cluster is to the question contexts.
When ΓQ consists of multiple contexts, we make the na¨ıve Bayes assumption that each individual context γQ ∈ ΓQ is independent of all other contexts given the cluster C. 
Equation (5) needs the parameters P(C|w) and P(in(C, γQ)|C), neither of which are directly available from the context-ﬁller database.
We will discuss the estimation of these parameters in Section 4.2. 
The previous model assigns the same likelihood to every instance of a given word.
As we noted in section 3.2.2, a word may be polysemous.
To take into account a word’s context, we can instead compute P(in(w, ΓQ)|w, in(w, Γw)), where Γw is the set of contexts for the candidate word w in a retrieved passage.
By introducing word clusters as intermediate variables as before and making a similar assumption as in equation (3), we obtain: 
Like equation (4), equation (7) partitions the model into two parts.
Unlike P(C|w) in equation (4), the probability of the cluster is now based on the particular occurrence of the word in the candidate contexts.
It can be estimated by: 
Our probabilistic model requires the parameters P(C|w), P(C|w, in(w, γ)), and P(in(C, γ)|C), where w is a word, C is a cluster that w belongs to, and γ is a question or candidate context.
This section explains how these parameters are estimated without using labeled data.
The context-ﬁller database described in Section 3.2 provides the joint and marginal frequency counts of contexts and words (|in(γ, w)|, |in(∗, γ) |and |in(w, ∗)|).
These counts allow us to compute the probabilities P(in(w, γ)), P(in(w, ∗)), and P(in(∗, γ)).
We can also compute P(in(w, γ)|w), which is smoothed with addone smoothing (see equation (11) in Figure 2).
The estimation of P(C|w) presents a challenge.
We have no corpus from which we can directly measure P(C|w) because word instances are not labeled with their clusters. 
We use the average weighted “guesses” of the top similar words of w to compute P(C|w) (see equation 13).
The intuition is that if w and w are similar words, P(C|w ) and P(C|w) tend to have similar values.
Since we do not know P(C|w ) either, we substitute it with uniform distribution Pu(C|w ) as in equation (12) of Figure 2.
Although Pu(C|w ) is a very crude guess, the weighted average of a set of such guesses can often be quite accurate.
The similarities between words are obtained as a byproduct of the CBC algorithm.
For each word, we use S(w) to denote the top-n most similar words (n=50 in our experiments) and sim(w, w ) to denote the similarity between words w and w .
The following is a sample similar word list for the 
plaint 0.29, lawsuits 0.27, jacket 0.25, countersuit 0.24, counterclaim 0.24, pants 0.24, trousers 0.22, shirt 0.21, slacks 0.21, case 0.21, pantsuit 0.21, shirts 0.20, sweater 0.20, coat 0.20, ...} The estimation for P(C|w, in(w, γw)) is similar to that of P(C|w) except that instead of all w ∈ S(w), we instead use {w |w ∈ S(w) ∧ in(w , γw)}.
By only looking at a particular context γw, we may obtain a different distribution over C than P(C|w) speciﬁes.
In the event that the data are too sparse to estimate P(C|w, in(w, γw)), we fall back to using P(C|w).
P(in(C, γ)|C) is computed in (14) by assuming each instance of w contains a fractional instance of C and the fractional count is P(C|w).
Again, add-one smoothing is used. 

2013-10-24 01:45:27.871936
conclusions
We have presented an unsupervised probabilistic answer type model.Our model uses contexts derived from the question and the candidate answer to calculate the appropriateness of a candidate answer.Statistics gathered from a large corpus of text are used in the calculation, and the model is constructed to exploit these statistics without being overly speciﬁc or overly general.The method presented here avoids the use of an explicit list of answer types.Explicit answer types can exhibit poor performance, especially for those questions not ﬁtting one of the types.They must also be redeﬁned when either the domain or corpus substantially changes.By avoiding their use, our answer typing method may be easier to adapt to different corpora and question answering domains (such as bioinformatics).In addition to operating as a stand-alone answer typing component, our system can be combined with other existing answer typing strategies, especially in situations in which a catch-all answer type is used.Our experimental results show that our probabilistic model outperforms the oracle and a system using automatic named entity recognition under such circumstances.The performance of our model is better than that of the semi-automatic system, which is a better indication of the expected performance of a comparable real-world answer typing system. introduction
Given a question, people are usually able to form an expectation about the type of the answer, even if they do not know the actual answer.An accurate expectation of the answer type makes it much easier to select the answer from a sentence that contains the query words.Consider the question “What is the capital of Norway?” We would expect the answer to be a city and could ﬁlter out most of the words in the following sentence: The landed aristocracy was virtually crushed by Hakon V, who reigned from 1299 to 1319, and Oslo became the capital of Norway, replacing Bergen as the principal city of the kingdom.The goal of answer typing is to determine whether a word’s semantic type is appropriate as an answer for a question.Many previous approaches to answer typing, e.g., (Ittycheriah et al., 2001; Li and Roth, 2002; Krishnan et al., 2005), employ a predeﬁned set of answer types and use supervised learning or manually constructed rules to classify a question according to expected answer type.A disadvantage of this approach is that there will always be questions whose answers do not belong to any of the predeﬁned types.Consider the question: “What are tourist attractions in Reims?” The answer may be many things: a church, a historic residence, a park, a famous intersection, a statue, etc.A common method to deal with this problem is to deﬁne a catch-all class.This class, however, tends not to be as effective as other answer types.Another disadvantage of predeﬁned answer types is with regard to granularity.If the types are too speciﬁc, they are more difﬁcult to tag.If they are too general, too many candidates may be identiﬁed as having the appropriate type.In contrast to previous approaches that use a supervised classiﬁer to categorize questions into a predeﬁned set of types, we propose an unsupervised method to dynamically construct a probabilistic answer type model for each question.Such a model can be used to evaluate whether or not a word ﬁts into the question context.For example, given the question “What are tourist attractions in Reims?”, we would expect the appropriate answers to ﬁt into the context “X is a tourist attraction.” From a corpus, we can ﬁnd the words that appeared in this context, such as: A-Ama Temple, Aborigine, addition, Anak Krakatau, archipelago, area, baseball, Bletchley Park, brewery, cabaret, Cairo, Cape Town, capital, center, ...Using the frequency counts of these words in the context, we construct a probabilistic model to compute P(in(w, Γ)|w), the probability for a word w to occur in a set of contexts Γ, given an occurrence of w. The parameters in this model are obtained from a large, automatically parsed, unlabeled corpus.By asking whether a word would occur in a particular context extracted from a question, we avoid explicitly specifying a list of possible answer types.This has the added beneﬁt of being easily adapted to different domains and corpora in which a list of explicit possible answer types may be difﬁcult to enumerate and/or identify within the text.The remainder of this paper is organized as follows.Section 2 discusses the work related to answer typing.Section 3 discusses some of the key concepts employed by our probabilistic model, including word clusters and the contexts of a question and a word.Section 4 presents our probabilistic model for answer typing.Section 5 compares the performance of our model with that of an oracle and a semi-automatic system performing the same task.Finally, the concluding remarks in are made in Section 6. abstract
All questions are implicitly associated with an expected answer type.Unlike previous approaches that require a predeﬁned set of question types, we present a method for dynamically constructing a probability-based answer type model for each different question.Our model evaluates the appropriateness of a potential answer by the probability that it ﬁts into the question contexts.Evaluation is performed against manual and semiautomatic methods using a ﬁxed set of answer labels.Results show our approach to be superior for those questions classiﬁed as having a miscellaneous answer type. acknowledgments
The authors would like to thank the anonymous reviewers for their helpful comments on improving the paper.The ﬁrst author is supported by the Natural Sciences and Engineering Research Council of Canada, the Alberta Ingenuity Fund, and the Alberta Informatics Circle of Research Excellence. related work
Light et al.(2001) performed an analysis of the effect of multiple answer type occurrences in a sentence.When multiple words of the same type appear in a sentence, answer typing with ﬁxed types must assign each the same score.Light et al.found that even with perfect answer sentence identiﬁcation, question typing, and semantic tagging, a system could only achieve 59% accuracy over the TREC-9 questions when using their set of 24 non-overlapping answer types.By computing the probability of an answer candidate occurring in the question contexts directly, we avoid having multiple candidates with the same level of appropriateness as answers.There have been a variety of approaches to determine the answer types, which are also known as Qtargets (Echihabi et al., 2003).Most previous approaches classify the answer type of a question as one of a set of predeﬁned types.Many systems construct the classiﬁcation rules manually (Cui et al., 2004; Greenwood, 2004; Hermjakob, 2001).The rules are usually triggered by the presence of certain words in the question.For example, if a question contains “author” then the expected answer type is Person.The number of answer types as well as the number of rules can vary a great deal.For example, (Hermjakob, 2001) used 276 rules for 122 answer types.Greenwood (2004), on the other hand, used 46 answer types with unspeciﬁed number of rules.The classiﬁcation rules can also be acquired with supervised learning.Ittycheriah, et al.(2001) describe a maximum entropy based question classiﬁcation scheme to classify each question as having one of the MUC answer types.In a similar experiment, Li & Roth (2002) train a question classiﬁer based on a modiﬁed version of SNoW using a richer set of answer types than Ittycheriah et al.The LCC system (Harabagiu et al., 2003) combines ﬁxed types with a novel loop-back strategy.In the event that a question cannot be classiﬁed as one of the ﬁxed entity types or semantic concepts derived from WordNet (Fellbaum, 1998), the answer type model backs off to a logic prover that uses axioms derived form WordNet, along with logic rules, to justify phrases as answers.Thus, the LCC system is able to avoid the use of a miscellaneous type that often exhibits poor performance.However, the logic prover must have sufﬁcient evidence to link the question to the answer, and general knowledge must be encoded as axioms into the system.In contrast, our answer type model derives all of its information automatically from unannotated text.Answer types are often used as ﬁlters.It was noted in (Radev et al., 2002) that a wrong guess about the answer type reduces the chance for the system to answer the question correctly by as much as 17 times.The approach presented here is less brittle.Even if the correct candidate does not have the highest likelihood according to the model, it may still be selected when the answer extraction module takes into account other factors such as the proximity to the matched keywords.Furthermore, a probabilistic model makes it easier to integrate the answer type scores with scores computed by other components in a question answering system in a principled fashion. evaluation
We evaluate our answer typing system by using it to ﬁlter the contents of documents retrieved by the information retrieval portion of a question answering system.Each answer candidate in the set of documents is scored by the answer typing system and the list is sorted in descending order of score.We treat the system as a ﬁlter and observe the proportion of candidates that must be accepted by the ﬁlter so that at least one correct answer is accepted.A model that allows a low percentage of candidates to pass while still allowing at least one correct answer through is favorable to a model in which a high number of candidates must pass.This represents an intrinsic rather than extrinsic evaluation (Moll´a and Hutchinson, 2003) that we believe illustrates the usefulness of our model.The evaluation data consist of 154 questions from the TREC-2003 QA Track (Voorhees, 2003) satisfying the following criteria, along with the top We compare the performance of our probabilistic model with that of two other systems.Both comparison systems make use of a small, predeﬁned set of manually-assigned MUC7 named-entity types (location, person, organization, cardinal, percent, date, time, duration, measure, money) augmented with thing-name (proper trec.nist.gov/data/qa/2003 qadata/03QA.tasks/t12.pats.txt names of inanimate objects) and miscellaneous (a catch-all answer type of all other candidates).Some examples of thing-name are Guinness Book of World Records, Thriller, Mars Pathﬁnder, and Grey Cup.Examples of miscellaneous answers are copper, oil, red, and iris.The differences in the comparison systems is with respect to how entity types are assigned to the words in the candidate documents.We make use of the ANNIE (Maynard et al., 2002) named entity recognition system, along with a manual assigned “oracle” strategy, to assign types to candidate answers.In each case, the score for a candidate is either 1 if it is tagged as the same type as the question or 0 otherwise.With this scoring scheme producing a sorted list we can compute the probability of the ﬁrst correct answer appearing at rank R = k as follows: where t is the number of unique candidate answers that are of the appropriate type and c is the number of unique candidate answers that are correct.Using the probabilities in equation (15), we compute the expected rank, E(R), of the ﬁrst correct answer of a given question in the system as: Answer candidates are the set of ANNIEidentiﬁed tokens with stop words and punctuation removed.This yields between 900 and 8000 candidates for each question, depending on the top 10 documents returned by PRISE.The oracle system represents an upper bound on using the predeﬁned set of answer types.The ANNIE system represents a more realistic expectation of performance.The median percentage of candidates that are accepted by a ﬁlter over the questions of our evaluation data provides one measure of performance and is preferred to the average because of the effect of large values on the average.In QA, a system accepting 60% of the candidates is not significantly better or worse than one accepting 100%, but the effect on average is quite high.Another measure is to observe the number of questions with at least one correct answer in the top N% for various values of N. By examining the number of correct answers found in the top N% we can better understand what an effective cutoff would be.The overall results of our comparison can be found in Table 2.We have added the results of a system that scores candidates based on their frequency within the document as a comparison with a simple, yet effective, strategy.The second column is the median percentage of where the highest scored correct answer appears in the sorted candidate list.Low percentage values mean the answer is usually found high in the sorted list.The remaining columns list the number of questions that have a correct answer somewhere in the top N% of their sorted lists.This is meant to show the effects of imposing a strict cutoff prior to running the answer type model.The oracle system performs best, as it beneﬁts from both manual question classiﬁcation and manual entity tagging.If entity assignment is performed by an automatic system (as it is for ANNIE), the performance drops noticeably.Our probabilistic model performs better than ANNIE and achieves approximately 2/3 of the performance of the oracle system.Table 2 also shows that the use of candidate contexts increases the performance of our answer type model.Table 3 shows the performance of the oracle system, our model, and the ANNIE system broken down by manually-assigned answer types.Due to insufﬁcient numbers of questions, the cardinal, percent, time, duration, measure, and money types are combined into an “Other” category.When compared with the oracle system, our model performs worse overall for questions of all types except for those seeking miscellaneous answers.For miscellaneous questions, the oracle identiﬁes all tokens that do not belong to one of the other known categories as possible answers.For all questions of non-miscellaneous type, only a small subset of the candidates are marked appropriate.In particular, our model performs worse than the oracle for questions seeking persons and thingnames.Person questions often seek rare person names, which occur in few contexts and are difﬁcult to reliably cluster.Thing-name questions are easy for a human to identify but difﬁcult for automatic system to identify.Thing-names are a diverse category and are not strongly associated with any identifying contexts.Our model outperforms the ANNIE system in general, and for questions seeking organizations, thing-names, and miscellaneous targets in particular.ANNIE may have low coverage on organization names, resulting in reduced performance.Like the oracle, ANNIE treats all candidates not assigned one of the categories as appropriate for miscellaneous questions.Because ANNIE cannot identify thing-names, they are treated as miscellaneous.ANNIE shows low performance on thingnames because words incorrectly assigned types are sorted to the bottom of the list for miscellaneous and thing-name questions.If a correct answer is incorrectly assigned a type it will be sorted near the bottom, resulting in a poor score. method
Before introducing our model, we ﬁrst describe the resources used in the model. Natural language data is extremely sparse.Word clusters are a way of coping with data sparseness by abstracting a given word to a class of related words.Clusters, as used by our probabilistic answer typing system, play a role similar to that of named entity types.Many methods exist for clustering, e.g., (Brown et al., 1990; Cutting et al., 1992; Pereira et al., 1993; Karypis et al., 1999).We used the Clustering By Committee (CBC) suite software, network, wireless, ... rooms, bathrooms, restrooms, ... meeting room, conference room, ... ghost rabbit, squirrel, duck, elephant, frog, ... goblins, ghosts, vampires, ghouls, ... punk, reggae, folk, pop, hip-pop, ... huge, larger, vast, signiﬁcant, ... coming-of-age, true-life, ... clouds, cloud, fog, haze, mist, ... algorithm (Pantel and Lin, 2002) on a 10 GB English text corpus to obtain 3607 clusters.The following is an example cluster generated by CBC: tension, anger, anxiety, tensions, frustration, resentment, uncertainty, confusion, conﬂict, discontent, insecurity, controversy, unease, bitterness, dispute, disagreement, nervousness, sadness, despair, animosity, hostility, outrage, discord, pessimism, anguish, ...In the clustering generated by CBC, a word may belong to multiple clusters.The clusters to which a word belongs often represent the senses of the word.Table 1 shows two example words and their clusters. The context in which a word appears often imposes constraints on the semantic type of the word.This basic idea has been exploited by many proposals for distributional similarity and clustering, e.g., (Church and Hanks, 1989; Lin, 1998; Pereira et al., 1993).Similar to Lin and Pantel (2001), we deﬁne the contexts of a word to be the undirected paths in dependency trees involving that word at either the beginning or the end.The following diagram shows an example dependency tree: Which city hosted the 1988 Winter Olympics? The links in the tree represent dependency relationships.The direction of a link is from the head to the modiﬁer in the relationship.Labels associated with the links represent types of relations.In a context, the word itself is replaced with a variable X.We say a word is the ﬁller of a context if it replaces X.For example, the contexts for the word “Olympics” in the above sentence include the following paths: In these paths, words are reduced to their root forms and proper names are reduced to their entity tags (we used MUC7 named entity tags).Paths allow us to balance the speciﬁcity of contexts and the sparseness of data.Longer paths typically impose stricter constraints on the slot ﬁllers.However, they tend to have fewer occurrences, making them more prone to errors arising from data sparseness.We have restricted the path length to two (involving at most three words) and require the two ends of the path to be nouns.We parsed the AQUAINT corpus (3GB) with Minipar (Lin, 2001) and collected the frequency counts of words appearing in various contexts.Parsing and database construction is performed off-line as the database is identical for all questions.We extracted 527,768 contexts that appeared at least 25 times in the corpus.An example context and its ﬁllers are shown in Figure 1. To build a probabilistic model for answer typing, we extract a set of contexts, called question contexts, from a question.An answer is expected to be a plausible ﬁller of the question contexts.Question contexts are extracted from a question with two rules.First, if the wh-word in a question has a trace in the parse tree, the question contexts are the contexts of the trace.For example, the question “What do most tourists visit in Reims?” is parsed as: Whati do most tourists visit ei in Reims? The symbol ei is the trace of whati.Minipar generates the trace to indicate that the word what is the object of visit in the deep structure of the sentence.The following question contexts are extracted from the above question: The second rule deals with situations where the wh-word is a determiner, as in the question “Which city hosted the 1988 Winter Olympics?” (the parse tree for which is shown in section 3.2).In such cases, the question contexts consist of a single context involving the noun that is modiﬁed by the determiner.The context for the above sentence is X city subj , corresponding to the sentence “X is a city.” This context is used because the question explicitly states that the desired answer is a city.The context overrides the other contexts because the question explicitly states the desired answer type.Experimental results have shown that using this context in conjunction with other contexts extracted from the question produces lower performance than using this context alone.In the event that a context extracted from a question is not found in the database, we shorten the context in one of two ways.We start by replacing the word at the end of the path with a wildcard that matches any word.If this fails to yield entries in the context database, we shorten the context to length one and replace the end word with automatically determined similar words instead of a wildcard. Candidate contexts are very similar in form to question contexts, save for one important difference.Candidate contexts are extracted from the parse trees of the answer candidates rather than the question.In natural language, some words may be polysemous.For example, Washington may refer to a person, a city, or a state.The occurrences of Washington in “Washington’s descendants” and “suburban Washington” should not be given the same score when the question is seeking a location.Given that the sense of a word is largely determined by its local context (Choueka and Lusignan, 1985), candidate contexts allow the model to take into account the candidate answers’ senses implicitly. The goal of an answer typing model is to evaluate the appropriateness of a candidate word as an answer to the question.If we assume that a set of answer candidates is provided to our model by some means (e.g., words comprising documents extracted by an information retrieval engine), we wish to compute the value P(in(w, ΓQ)|w).That is, the appropriateness of a candidate answer w is proportional to the probability that it will occur in the question contexts ΓQ extracted from the question.To mitigate data sparseness, we can introduce a hidden variable C that represents the clusters to which the candidate answer may belong.As a candidate may belong to multiple clusters, we obtain: Given that a word appears, we assume that it has the same probability to appear in a context as all other words in the same cluster.Therefore: We can now rewrite the equation in (2) as: This equation splits our model into two parts: one models which clusters a word belongs to and the other models how appropriate a cluster is to the question contexts.When ΓQ consists of multiple contexts, we make the na¨ıve Bayes assumption that each individual context γQ ∈ ΓQ is independent of all other contexts given the cluster C. Equation (5) needs the parameters P(C|w) and P(in(C, γQ)|C), neither of which are directly available from the context-ﬁller database.We will discuss the estimation of these parameters in Section 4.2. The previous model assigns the same likelihood to every instance of a given word.As we noted in section 3.2.2, a word may be polysemous.To take into account a word’s context, we can instead compute P(in(w, ΓQ)|w, in(w, Γw)), where Γw is the set of contexts for the candidate word w in a retrieved passage.By introducing word clusters as intermediate variables as before and making a similar assumption as in equation (3), we obtain: Like equation (4), equation (7) partitions the model into two parts.Unlike P(C|w) in equation (4), the probability of the cluster is now based on the particular occurrence of the word in the candidate contexts.It can be estimated by: Our probabilistic model requires the parameters P(C|w), P(C|w, in(w, γ)), and P(in(C, γ)|C), where w is a word, C is a cluster that w belongs to, and γ is a question or candidate context.This section explains how these parameters are estimated without using labeled data.The context-ﬁller database described in Section 3.2 provides the joint and marginal frequency counts of contexts and words (|in(γ, w)|, |in(∗, γ) |and |in(w, ∗)|).These counts allow us to compute the probabilities P(in(w, γ)), P(in(w, ∗)), and P(in(∗, γ)).We can also compute P(in(w, γ)|w), which is smoothed with addone smoothing (see equation (11) in Figure 2).The estimation of P(C|w) presents a challenge.We have no corpus from which we can directly measure P(C|w) because word instances are not labeled with their clusters. We use the average weighted “guesses” of the top similar words of w to compute P(C|w) (see equation 13).The intuition is that if w and w are similar words, P(C|w ) and P(C|w) tend to have similar values.Since we do not know P(C|w ) either, we substitute it with uniform distribution Pu(C|w ) as in equation (12) of Figure 2.Although Pu(C|w ) is a very crude guess, the weighted average of a set of such guesses can often be quite accurate.The similarities between words are obtained as a byproduct of the CBC algorithm.For each word, we use S(w) to denote the top-n most similar words (n=50 in our experiments) and sim(w, w ) to denote the similarity between words w and w .The following is a sample similar word list for the plaint 0.29, lawsuits 0.27, jacket 0.25, countersuit 0.24, counterclaim 0.24, pants 0.24, trousers 0.22, shirt 0.21, slacks 0.21, case 0.21, pantsuit 0.21, shirts 0.20, sweater 0.20, coat 0.20, ...} The estimation for P(C|w, in(w, γw)) is similar to that of P(C|w) except that instead of all w ∈ S(w), we instead use {w |w ∈ S(w) ∧ in(w , γw)}.By only looking at a particular context γw, we may obtain a different distribution over C than P(C|w) speciﬁes.In the event that the data are too sparse to estimate P(C|w, in(w, γw)), we fall back to using P(C|w).P(in(C, γ)|C) is computed in (14) by assuming each instance of w contains a fractional instance of C and the fractional count is P(C|w).Again, add-one smoothing is used. 

2013-10-24 01:49:17.564249

conclusions
We have presented an unsupervised probabilistic answer type model.Our model uses contexts derived from the question and the candidate answer to calculate the appropriateness of a candidate answer.Statistics gathered from a large corpus of text are used in the calculation, and the model is constructed to exploit these statistics without being overly speciﬁc or overly general.The method presented here avoids the use of an explicit list of answer types.Explicit answer types can exhibit poor performance, especially for those questions not ﬁtting one of the types.They must also be redeﬁned when either the domain or corpus substantially changes.By avoiding their use, our answer typing method may be easier to adapt to different corpora and question answering domains (such as bioinformatics).In addition to operating as a stand-alone answer typing component, our system can be combined with other existing answer typing strategies, especially in situations in which a catch-all answer type is used.Our experimental results show that our probabilistic model outperforms the oracle and a system using automatic named entity recognition under such circumstances.The performance of our model is better than that of the semi-automatic system, which is a better indication of the expected performance of a comparable real-world answer typing system. 
introduction
Given a question, people are usually able to form an expectation about the type of the answer, even if they do not know the actual answer.An accurate expectation of the answer type makes it much easier to select the answer from a sentence that contains the query words.Consider the question “What is the capital of Norway?” We would expect the answer to be a city and could ﬁlter out most of the words in the following sentence: The landed aristocracy was virtually crushed by Hakon V, who reigned from 1299 to 1319, and Oslo became the capital of Norway, replacing Bergen as the principal city of the kingdom.The goal of answer typing is to determine whether a word’s semantic type is appropriate as an answer for a question.Many previous approaches to answer typing, e.g., (Ittycheriah et al., 2001; Li and Roth, 2002; Krishnan et al., 2005), employ a predeﬁned set of answer types and use supervised learning or manually constructed rules to classify a question according to expected answer type.A disadvantage of this approach is that there will always be questions whose answers do not belong to any of the predeﬁned types.Consider the question: “What are tourist attractions in Reims?” The answer may be many things: a church, a historic residence, a park, a famous intersection, a statue, etc.A common method to deal with this problem is to deﬁne a catch-all class.This class, however, tends not to be as effective as other answer types.Another disadvantage of predeﬁned answer types is with regard to granularity.If the types are too speciﬁc, they are more difﬁcult to tag.If they are too general, too many candidates may be identiﬁed as having the appropriate type.In contrast to previous approaches that use a supervised classiﬁer to categorize questions into a predeﬁned set of types, we propose an unsupervised method to dynamically construct a probabilistic answer type model for each question.Such a model can be used to evaluate whether or not a word ﬁts into the question context.For example, given the question “What are tourist attractions in Reims?”, we would expect the appropriate answers to ﬁt into the context “X is a tourist attraction.” From a corpus, we can ﬁnd the words that appeared in this context, such as: A-Ama Temple, Aborigine, addition, Anak Krakatau, archipelago, area, baseball, Bletchley Park, brewery, cabaret, Cairo, Cape Town, capital, center, ...Using the frequency counts of these words in the context, we construct a probabilistic model to compute P(in(w, Γ)|w), the probability for a word w to occur in a set of contexts Γ, given an occurrence of w. The parameters in this model are obtained from a large, automatically parsed, unlabeled corpus.By asking whether a word would occur in a particular context extracted from a question, we avoid explicitly specifying a list of possible answer types.This has the added beneﬁt of being easily adapted to different domains and corpora in which a list of explicit possible answer types may be difﬁcult to enumerate and/or identify within the text.The remainder of this paper is organized as follows.Section 2 discusses the work related to answer typing.Section 3 discusses some of the key concepts employed by our probabilistic model, including word clusters and the contexts of a question and a word.Section 4 presents our probabilistic model for answer typing.Section 5 compares the performance of our model with that of an oracle and a semi-automatic system performing the same task.Finally, the concluding remarks in are made in Section 6. 
abstract
All questions are implicitly associated with an expected answer type.Unlike previous approaches that require a predeﬁned set of question types, we present a method for dynamically constructing a probability-based answer type model for each different question.Our model evaluates the appropriateness of a potential answer by the probability that it ﬁts into the question contexts.Evaluation is performed against manual and semiautomatic methods using a ﬁxed set of answer labels.Results show our approach to be superior for those questions classiﬁed as having a miscellaneous answer type. 
acknowledgments
The authors would like to thank the anonymous reviewers for their helpful comments on improving the paper.The ﬁrst author is supported by the Natural Sciences and Engineering Research Council of Canada, the Alberta Ingenuity Fund, and the Alberta Informatics Circle of Research Excellence. 
related work
Light et al.(2001) performed an analysis of the effect of multiple answer type occurrences in a sentence.When multiple words of the same type appear in a sentence, answer typing with ﬁxed types must assign each the same score.Light et al.found that even with perfect answer sentence identiﬁcation, question typing, and semantic tagging, a system could only achieve 59% accuracy over the TREC-9 questions when using their set of 24 non-overlapping answer types.By computing the probability of an answer candidate occurring in the question contexts directly, we avoid having multiple candidates with the same level of appropriateness as answers.There have been a variety of approaches to determine the answer types, which are also known as Qtargets (Echihabi et al., 2003).Most previous approaches classify the answer type of a question as one of a set of predeﬁned types.Many systems construct the classiﬁcation rules manually (Cui et al., 2004; Greenwood, 2004; Hermjakob, 2001).The rules are usually triggered by the presence of certain words in the question.For example, if a question contains “author” then the expected answer type is Person.The number of answer types as well as the number of rules can vary a great deal.For example, (Hermjakob, 2001) used 276 rules for 122 answer types.Greenwood (2004), on the other hand, used 46 answer types with unspeciﬁed number of rules.The classiﬁcation rules can also be acquired with supervised learning.Ittycheriah, et al.(2001) describe a maximum entropy based question classiﬁcation scheme to classify each question as having one of the MUC answer types.In a similar experiment, Li & Roth (2002) train a question classiﬁer based on a modiﬁed version of SNoW using a richer set of answer types than Ittycheriah et al.The LCC system (Harabagiu et al., 2003) combines ﬁxed types with a novel loop-back strategy.In the event that a question cannot be classiﬁed as one of the ﬁxed entity types or semantic concepts derived from WordNet (Fellbaum, 1998), the answer type model backs off to a logic prover that uses axioms derived form WordNet, along with logic rules, to justify phrases as answers.Thus, the LCC system is able to avoid the use of a miscellaneous type that often exhibits poor performance.However, the logic prover must have sufﬁcient evidence to link the question to the answer, and general knowledge must be encoded as axioms into the system.In contrast, our answer type model derives all of its information automatically from unannotated text.Answer types are often used as ﬁlters.It was noted in (Radev et al., 2002) that a wrong guess about the answer type reduces the chance for the system to answer the question correctly by as much as 17 times.The approach presented here is less brittle.Even if the correct candidate does not have the highest likelihood according to the model, it may still be selected when the answer extraction module takes into account other factors such as the proximity to the matched keywords.Furthermore, a probabilistic model makes it easier to integrate the answer type scores with scores computed by other components in a question answering system in a principled fashion. 
evaluation
We evaluate our answer typing system by using it to ﬁlter the contents of documents retrieved by the information retrieval portion of a question answering system.Each answer candidate in the set of documents is scored by the answer typing system and the list is sorted in descending order of score.We treat the system as a ﬁlter and observe the proportion of candidates that must be accepted by the ﬁlter so that at least one correct answer is accepted.A model that allows a low percentage of candidates to pass while still allowing at least one correct answer through is favorable to a model in which a high number of candidates must pass.This represents an intrinsic rather than extrinsic evaluation (Moll´a and Hutchinson, 2003) that we believe illustrates the usefulness of our model.The evaluation data consist of 154 questions from the TREC-2003 QA Track (Voorhees, 2003) satisfying the following criteria, along with the top We compare the performance of our probabilistic model with that of two other systems.Both comparison systems make use of a small, predeﬁned set of manually-assigned MUC7 named-entity types (location, person, organization, cardinal, percent, date, time, duration, measure, money) augmented with thing-name (proper trec.nist.gov/data/qa/2003 qadata/03QA.tasks/t12.pats.txt names of inanimate objects) and miscellaneous (a catch-all answer type of all other candidates).Some examples of thing-name are Guinness Book of World Records, Thriller, Mars Pathﬁnder, and Grey Cup.Examples of miscellaneous answers are copper, oil, red, and iris.The differences in the comparison systems is with respect to how entity types are assigned to the words in the candidate documents.We make use of the ANNIE (Maynard et al., 2002) named entity recognition system, along with a manual assigned “oracle” strategy, to assign types to candidate answers.In each case, the score for a candidate is either 1 if it is tagged as the same type as the question or 0 otherwise.With this scoring scheme producing a sorted list we can compute the probability of the ﬁrst correct answer appearing at rank R = k as follows: where t is the number of unique candidate answers that are of the appropriate type and c is the number of unique candidate answers that are correct.Using the probabilities in equation (15), we compute the expected rank, E(R), of the ﬁrst correct answer of a given question in the system as: Answer candidates are the set of ANNIEidentiﬁed tokens with stop words and punctuation removed.This yields between 900 and 8000 candidates for each question, depending on the top 10 documents returned by PRISE.The oracle system represents an upper bound on using the predeﬁned set of answer types.The ANNIE system represents a more realistic expectation of performance.The median percentage of candidates that are accepted by a ﬁlter over the questions of our evaluation data provides one measure of performance and is preferred to the average because of the effect of large values on the average.In QA, a system accepting 60% of the candidates is not significantly better or worse than one accepting 100%, but the effect on average is quite high.Another measure is to observe the number of questions with at least one correct answer in the top N% for various values of N. By examining the number of correct answers found in the top N% we can better understand what an effective cutoff would be.The overall results of our comparison can be found in Table 2.We have added the results of a system that scores candidates based on their frequency within the document as a comparison with a simple, yet effective, strategy.The second column is the median percentage of where the highest scored correct answer appears in the sorted candidate list.Low percentage values mean the answer is usually found high in the sorted list.The remaining columns list the number of questions that have a correct answer somewhere in the top N% of their sorted lists.This is meant to show the effects of imposing a strict cutoff prior to running the answer type model.The oracle system performs best, as it beneﬁts from both manual question classiﬁcation and manual entity tagging.If entity assignment is performed by an automatic system (as it is for ANNIE), the performance drops noticeably.Our probabilistic model performs better than ANNIE and achieves approximately 2/3 of the performance of the oracle system.Table 2 also shows that the use of candidate contexts increases the performance of our answer type model.Table 3 shows the performance of the oracle system, our model, and the ANNIE system broken down by manually-assigned answer types.Due to insufﬁcient numbers of questions, the cardinal, percent, time, duration, measure, and money types are combined into an “Other” category.When compared with the oracle system, our model performs worse overall for questions of all types except for those seeking miscellaneous answers.For miscellaneous questions, the oracle identiﬁes all tokens that do not belong to one of the other known categories as possible answers.For all questions of non-miscellaneous type, only a small subset of the candidates are marked appropriate.In particular, our model performs worse than the oracle for questions seeking persons and thingnames.Person questions often seek rare person names, which occur in few contexts and are difﬁcult to reliably cluster.Thing-name questions are easy for a human to identify but difﬁcult for automatic system to identify.Thing-names are a diverse category and are not strongly associated with any identifying contexts.Our model outperforms the ANNIE system in general, and for questions seeking organizations, thing-names, and miscellaneous targets in particular.ANNIE may have low coverage on organization names, resulting in reduced performance.Like the oracle, ANNIE treats all candidates not assigned one of the categories as appropriate for miscellaneous questions.Because ANNIE cannot identify thing-names, they are treated as miscellaneous.ANNIE shows low performance on thingnames because words incorrectly assigned types are sorted to the bottom of the list for miscellaneous and thing-name questions.If a correct answer is incorrectly assigned a type it will be sorted near the bottom, resulting in a poor score. 
method
Before introducing our model, we ﬁrst describe the resources used in the model. Natural language data is extremely sparse.Word clusters are a way of coping with data sparseness by abstracting a given word to a class of related words.Clusters, as used by our probabilistic answer typing system, play a role similar to that of named entity types.Many methods exist for clustering, e.g., (Brown et al., 1990; Cutting et al., 1992; Pereira et al., 1993; Karypis et al., 1999).We used the Clustering By Committee (CBC) suite software, network, wireless, ... rooms, bathrooms, restrooms, ... meeting room, conference room, ... ghost rabbit, squirrel, duck, elephant, frog, ... goblins, ghosts, vampires, ghouls, ... punk, reggae, folk, pop, hip-pop, ... huge, larger, vast, signiﬁcant, ... coming-of-age, true-life, ... clouds, cloud, fog, haze, mist, ... algorithm (Pantel and Lin, 2002) on a 10 GB English text corpus to obtain 3607 clusters.The following is an example cluster generated by CBC: tension, anger, anxiety, tensions, frustration, resentment, uncertainty, confusion, conﬂict, discontent, insecurity, controversy, unease, bitterness, dispute, disagreement, nervousness, sadness, despair, animosity, hostility, outrage, discord, pessimism, anguish, ...In the clustering generated by CBC, a word may belong to multiple clusters.The clusters to which a word belongs often represent the senses of the word.Table 1 shows two example words and their clusters. The context in which a word appears often imposes constraints on the semantic type of the word.This basic idea has been exploited by many proposals for distributional similarity and clustering, e.g., (Church and Hanks, 1989; Lin, 1998; Pereira et al., 1993).Similar to Lin and Pantel (2001), we deﬁne the contexts of a word to be the undirected paths in dependency trees involving that word at either the beginning or the end.The following diagram shows an example dependency tree: Which city hosted the 1988 Winter Olympics? The links in the tree represent dependency relationships.The direction of a link is from the head to the modiﬁer in the relationship.Labels associated with the links represent types of relations.In a context, the word itself is replaced with a variable X.We say a word is the ﬁller of a context if it replaces X.For example, the contexts for the word “Olympics” in the above sentence include the following paths: In these paths, words are reduced to their root forms and proper names are reduced to their entity tags (we used MUC7 named entity tags).Paths allow us to balance the speciﬁcity of contexts and the sparseness of data.Longer paths typically impose stricter constraints on the slot ﬁllers.However, they tend to have fewer occurrences, making them more prone to errors arising from data sparseness.We have restricted the path length to two (involving at most three words) and require the two ends of the path to be nouns.We parsed the AQUAINT corpus (3GB) with Minipar (Lin, 2001) and collected the frequency counts of words appearing in various contexts.Parsing and database construction is performed off-line as the database is identical for all questions.We extracted 527,768 contexts that appeared at least 25 times in the corpus.An example context and its ﬁllers are shown in Figure 1. To build a probabilistic model for answer typing, we extract a set of contexts, called question contexts, from a question.An answer is expected to be a plausible ﬁller of the question contexts.Question contexts are extracted from a question with two rules.First, if the wh-word in a question has a trace in the parse tree, the question contexts are the contexts of the trace.For example, the question “What do most tourists visit in Reims?” is parsed as: Whati do most tourists visit ei in Reims? The symbol ei is the trace of whati.Minipar generates the trace to indicate that the word what is the object of visit in the deep structure of the sentence.The following question contexts are extracted from the above question: The second rule deals with situations where the wh-word is a determiner, as in the question “Which city hosted the 1988 Winter Olympics?” (the parse tree for which is shown in section 3.2).In such cases, the question contexts consist of a single context involving the noun that is modiﬁed by the determiner.The context for the above sentence is X city subj , corresponding to the sentence “X is a city.” This context is used because the question explicitly states that the desired answer is a city.The context overrides the other contexts because the question explicitly states the desired answer type.Experimental results have shown that using this context in conjunction with other contexts extracted from the question produces lower performance than using this context alone.In the event that a context extracted from a question is not found in the database, we shorten the context in one of two ways.We start by replacing the word at the end of the path with a wildcard that matches any word.If this fails to yield entries in the context database, we shorten the context to length one and replace the end word with automatically determined similar words instead of a wildcard. Candidate contexts are very similar in form to question contexts, save for one important difference.Candidate contexts are extracted from the parse trees of the answer candidates rather than the question.In natural language, some words may be polysemous.For example, Washington may refer to a person, a city, or a state.The occurrences of Washington in “Washington’s descendants” and “suburban Washington” should not be given the same score when the question is seeking a location.Given that the sense of a word is largely determined by its local context (Choueka and Lusignan, 1985), candidate contexts allow the model to take into account the candidate answers’ senses implicitly. The goal of an answer typing model is to evaluate the appropriateness of a candidate word as an answer to the question.If we assume that a set of answer candidates is provided to our model by some means (e.g., words comprising documents extracted by an information retrieval engine), we wish to compute the value P(in(w, ΓQ)|w).That is, the appropriateness of a candidate answer w is proportional to the probability that it will occur in the question contexts ΓQ extracted from the question.To mitigate data sparseness, we can introduce a hidden variable C that represents the clusters to which the candidate answer may belong.As a candidate may belong to multiple clusters, we obtain: Given that a word appears, we assume that it has the same probability to appear in a context as all other words in the same cluster.Therefore: We can now rewrite the equation in (2) as: This equation splits our model into two parts: one models which clusters a word belongs to and the other models how appropriate a cluster is to the question contexts.When ΓQ consists of multiple contexts, we make the na¨ıve Bayes assumption that each individual context γQ ∈ ΓQ is independent of all other contexts given the cluster C. Equation (5) needs the parameters P(C|w) and P(in(C, γQ)|C), neither of which are directly available from the context-ﬁller database.We will discuss the estimation of these parameters in Section 4.2. The previous model assigns the same likelihood to every instance of a given word.As we noted in section 3.2.2, a word may be polysemous.To take into account a word’s context, we can instead compute P(in(w, ΓQ)|w, in(w, Γw)), where Γw is the set of contexts for the candidate word w in a retrieved passage.By introducing word clusters as intermediate variables as before and making a similar assumption as in equation (3), we obtain: Like equation (4), equation (7) partitions the model into two parts.Unlike P(C|w) in equation (4), the probability of the cluster is now based on the particular occurrence of the word in the candidate contexts.It can be estimated by: Our probabilistic model requires the parameters P(C|w), P(C|w, in(w, γ)), and P(in(C, γ)|C), where w is a word, C is a cluster that w belongs to, and γ is a question or candidate context.This section explains how these parameters are estimated without using labeled data.The context-ﬁller database described in Section 3.2 provides the joint and marginal frequency counts of contexts and words (|in(γ, w)|, |in(∗, γ) |and |in(w, ∗)|).These counts allow us to compute the probabilities P(in(w, γ)), P(in(w, ∗)), and P(in(∗, γ)).We can also compute P(in(w, γ)|w), which is smoothed with addone smoothing (see equation (11) in Figure 2).The estimation of P(C|w) presents a challenge.We have no corpus from which we can directly measure P(C|w) because word instances are not labeled with their clusters. We use the average weighted “guesses” of the top similar words of w to compute P(C|w) (see equation 13).The intuition is that if w and w are similar words, P(C|w ) and P(C|w) tend to have similar values.Since we do not know P(C|w ) either, we substitute it with uniform distribution Pu(C|w ) as in equation (12) of Figure 2.Although Pu(C|w ) is a very crude guess, the weighted average of a set of such guesses can often be quite accurate.The similarities between words are obtained as a byproduct of the CBC algorithm.For each word, we use S(w) to denote the top-n most similar words (n=50 in our experiments) and sim(w, w ) to denote the similarity between words w and w .The following is a sample similar word list for the plaint 0.29, lawsuits 0.27, jacket 0.25, countersuit 0.24, counterclaim 0.24, pants 0.24, trousers 0.22, shirt 0.21, slacks 0.21, case 0.21, pantsuit 0.21, shirts 0.20, sweater 0.20, coat 0.20, ...} The estimation for P(C|w, in(w, γw)) is similar to that of P(C|w) except that instead of all w ∈ S(w), we instead use {w |w ∈ S(w) ∧ in(w , γw)}.By only looking at a particular context γw, we may obtain a different distribution over C than P(C|w) speciﬁes.In the event that the data are too sparse to estimate P(C|w, in(w, γw)), we fall back to using P(C|w).P(in(C, γ)|C) is computed in (14) by assuming each instance of w contains a fractional instance of C and the fractional count is P(C|w).Again, add-one smoothing is used. 
