
2013-11-24 21:49:22.993008
C08-1122

2013-11-24 21:49:22.993102
All Sentences

2013-11-24 21:49:22.993203
Given a document cluster, CollabRank makes use of the global word relationships in the cluster to evaluate and rank candidate phrases for each single document in the cluster based on the graph-based ranking algorithm. [0.00868840809792]
Instead of making only use of the word relationships in a single document, the algorithm can incorporate the “voting” or “recommendations” between words in all the documents of the cluster, thus making use of the global information existing in the cluster context. [0.00859020597604]
All the candidate phrases in the document are ranked in decreasing order of the phrase scores and the top n phrases are selected as the keyphrases of the document. [0.00848024778439]
In this study, the cluster context is obtained by applying the clustering algorithm on the document set, and we have investigated how the cluster context influences the keyphrase extraction performance by employing different clustering algorithms. [0.00829185777805]
It is noteworthy that in addition to the graphbased ranking algorithm, other keyphrase extraction methods can also be integrated in the proposed collaborative framework to exploit the collaborative knowledge in the cluster context.  [0.00815372045375]

2013-11-24 21:50:36.035407
C08-1122

2013-11-24 21:50:36.035493
Filtered Sentences

2013-11-24 21:50:36.035594
CollabRank is implemented by first employing the clustering algorithm to obtain appropriate document clusters, and then using the graph-based ranking algorithm for collaborative single-document keyphrase extraction within each cluster. [0.0109343286012]
Given a document set for keyphrase extraction of each single document, CollabRank first employs the clustering algorithm to group the documents into a few clusters. [0.0108741932147]
Given a document cluster, CollabRank makes use of the global word relationships in the cluster to evaluate and rank candidate phrases for each single document in the cluster based on the graph-based ranking algorithm. [0.0105224459204]
In this study, the cluster context is obtained by applying the clustering algorithm on the document set, and we have investigated how the cluster context influences the keyphrase extraction performance by employing different clustering algorithms. [0.00994817101112]
Different clustering results lead to different document relationships and a high-quality cluster produced by popular algorithms is deemed to build an appropriate cluster context for collaborative keyphrase extraction.  [0.00886212950108]

2013-11-24 21:54:30.516493
P07-3014

2013-11-24 21:54:30.516575
All Sentences

2013-11-24 21:54:30.516681
The motivation for this paper is to discover which joining terms are good predictors of a semantic relation, and which learning algorithms perform best at the task of mapping from joining terms to semantic relations for modifier-noun compounds.  [0.0100782551263]
Also, they use 64 joining terms and gather counts for both the forms &quot;noun joining term modifier&quot; and &quot;modifier joining term noun&quot; (128 frequencies in total); while we use only the former construction with 28 joining terms. [0.00995469929472]
Turney and Littman (2005) use a set of 64 short prepositional and conjunctive phrases they call &quot;joining terms&quot; to generate exact queries for AltaVista of the form &quot;noun joining term modifier&quot;, and &quot;modifier joining term noun&quot;. [0.00978660832955]
The attributes used as input to the learning algorithms are the web frequencies for phrases containing the modifier, noun, and a prepositional joining term. [0.00968669598296]
We did not collect queries of the form &quot;modifier joining term head&quot;; in the majority of paraphrases of noun phrases the head noun occurs before the modifying word. [0.00952768641964]

2013-11-24 21:55:07.700525
P07-3014

2013-11-24 21:55:07.700612
Filtered Sentences

2013-11-24 21:55:07.700713
Also, they use 64 joining terms and gather counts for both the forms &quot;noun joining term modifier&quot; and &quot;modifier joining term noun&quot; (128 frequencies in total); while we use only the former construction with 28 joining terms. [0.0139733414543]
Turney and Littman (2005) use a set of 64 short prepositional and conjunctive phrases they call &quot;joining terms&quot; to generate exact queries for AltaVista of the form &quot;noun joining term modifier&quot;, and &quot;modifier joining term noun&quot;. [0.0137457822279]
The motivation for this paper is to discover which joining terms are good predictors of a semantic relation, and which learning algorithms perform best at the task of mapping from joining terms to semantic relations for modifier-noun compounds.  [0.0116706525042]
One method for deducing semantic relations between words in compounds involves gathering n-gram frequencies of these paraphrases, containing a noun, a modifier and a &quot;joining term&quot; that links them. [0.0114824305952]
Of the 600 modifier-noun phrases, three contained hyphenated or two-word modifier terms, for example &quot;test-tube baby&quot;. [0.0114673746654]

2013-11-24 21:57:26.489042
P10-1024

2013-11-24 21:57:26.489137
All Sentences

2013-11-24 21:57:26.489238
This is an indication that the collocation between the argument and the preposition is more indicative of the core/adjunct label than the obligatoriness of the slot (as expressed by the predicate-slot collocation). [0.0061075306658]
We define three measures, one quantifying the obligatoriness of the slot, another quantifying the selectional preference of the verb to the argument and a third that quantifies the association between the head word and the slot irrespective of the predicate (Section 3.3). [0.00608364194187]
In order not to bias the counts towards predicates which tend to take more arguments, we define here N(p, s) to be the number of times the (p, s) pair occurred in the training corpus, irrespective of the number of head words the argument had (and not e.g., EhN(p, s, h)). [0.00557730144252]
Core ∗ Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. [0.00538712104861]
In order to avoid tuning a parameter for each of the measures, we set the threshold as the median value of this measure in the test set. [0.00517638980086]

2013-11-24 21:57:52.321945
P10-1024

2013-11-24 21:57:52.322024
Filtered Sentences

2013-11-24 21:57:52.322115
Given a (predicate, prepositional argument) pair from the test set, we first tag and parse the argument using the unsupervised tools above5. [0.00570584324259]
We also report results for partial versions of the algorithm, starting with the three measures used (selectional preference, predicate-slot collocation and argument-slot collocation). [0.00568814614766]
The success of supervised methods stems from the fact that the predicate-slot combination (slot is represented in this paper by its preposition) strongly determines whether a given argument is an adjunct or a core (see Section 3.4). [0.00555655964524]
The measures used are based on selectional preference, predicate-slot collocation and argument-slot collocation. [0.00547947433569]
Non-prepositional arguments are invariably tagged as cores and out of coverage prepositional arguments as adjuncts. [0.00545327094923]

2013-11-24 21:58:58.230721
P99-1026

2013-11-24 21:58:58.230801
All Sentences

2013-11-24 21:58:58.230894
In Japanese dialogue systems, producing a backchannel is effective when the user's intention is not clear at that point in time, but determining the content of responses in a real-time spoken dialogue system is also beyond the scope of this paper.  [0.00895602117143]
The contexts created by the utterance understanding module can also be accessed by the response generation module so that it can produce responses based on the belief state in the context with the highest priority at a point in time. [0.00869934786994]
The response generation module is invoked when the user pauses, and plans responses based on the belief state of the context with the highest priority. [0.00867846359028]
A significant utterance (SU) in the user's speech is a phrase that plays a crucial role in performing the task in the dialogue. [0.0085382907522]
A meaningful discussion on the success rate of utterance segmentation is not possible because of the recognition errors due to the small coverage of the recognition grammar.3 All subjects successfully completed the task with system A in an average of 42.5 seconds, and six subjects did so with system B in an average of 55.0 seconds. [0.00837203192315]

2013-11-24 21:59:15.495175
P99-1026

2013-11-24 21:59:15.495256
Filtered Sentences

2013-11-24 21:59:15.495357
This paper proposed ISSS (incremental significantutterance-sequence search), an integrated incremental parsing and discourse processing method that enables both the understanding of unsegmented user utterances and real-time responses. [0.0090008864185]
In Japanese dialogue systems, producing a backchannel is effective when the user's intention is not clear at that point in time, but determining the content of responses in a real-time spoken dialogue system is also beyond the scope of this paper.  [0.00886579975626]
The above problem can be solved by incremental understanding, which means obtaining the most plausible interpretation of user utterances every time a word hypothesis is inputted from the speech recognizer. [0.00870629205636]
The contexts created by the utterance understanding module can also be accessed by the response generation module so that it can produce responses based on the belief state in the context with the highest priority at a point in time. [0.00867230995941]
The newest word hypothesis determines the word sequence that is acoustically most likely at a point in time.2 The utterance understanding module works based on ISSS and uses a domain-dependent unification grammar with a context-free backbone that is based on bunsetsu phrases. [0.00844744982784]

2013-11-24 22:00:24.267625
W10-1919

2013-11-24 22:00:24.267731
All Sentences

2013-11-24 22:00:24.267891
As part of the present study, we introduce annotation for gene/gene product (GGP) mentions (Section 3.2), and in the following discussion of applying an event extraction approach to the domain the availability of this class annotation as an additional category is assumed.  [0.00933524445126]
As related types of statements are annotated as Localization events in the applied model, we propose to apply this event type and differentiate between the specific subtypes on the basis of the event arguments. [0.00932068859508]
As a demonstration of feasibility the result is encouraging for both the applicability of event extraction to this specific new domain and for the adaptability of the approach to new domains in general.  [0.00906405113421]
We applied a previously introduced corpus of subdomain full texts annotated for mentions of bacteria and terms from the three top-level Gene Ontology subontologies as a reference defining domain information needs to study how these can be met through the application of events defined in the BioNLP’09 Shared Task on event extraction. [0.00897200216539]
These constraints assure that the corpus is relevant to the information needs of biologists working in the domain and that it can be used as a reference for the study of automatic GO annotation. [0.00888443703201]

2013-11-24 22:00:38.132065
W10-1919

2013-11-24 22:00:38.132161
Filtered Sentences

2013-11-24 22:00:38.132275
We applied a previously introduced corpus of subdomain full texts annotated for mentions of bacteria and terms from the three top-level Gene Ontology subontologies as a reference defining domain information needs to study how these can be met through the application of events defined in the BioNLP’09 Shared Task on event extraction. [0.0106147676946]
For event extraction, we applied the BioNLP’09 shared task event extraction criteria (Kim et al., 2009) with one key change: to make it possible to evaluate the extraction of the high-level process participants, we removed the requirement that all events must define a Theme as their core argument.  [0.00969382227473]
The BioNLP’09 shared task defined nine event types and five argument types (roles): Theme specifies the core participant(s) that an event affects, Cause the cause of the the event, Site a specific domain or region on a participant involved in the event, and ToLoc and AtLoc locations associated with localization events (Table 1). [0.00961255252395]
As gene and gene product entities are central to domain information needs and the core entities of the applied event extraction approach, we first introduced annotation for this entity class. [0.00953743205502]
During annotation, the number of annotated GGP associations with the targeted class of processes in the T4SS subcorpus was found to be too low to provide material for both training and testing a supervised learning-based event extraction approach. [0.00932628629813]

2013-11-24 23:17:51.914557
W93-0225

2013-11-24 23:17:51.914667
All Sentences

2013-11-24 23:17:51.914778
Moore and Pollack (1992) propose that, in fact, two contiguous spans of text may be in both an informational and an intentional relation simultaneously and that recognition of one kind of relation can facilitate recognition of the other. [0.0164622456991]
As noted above, nuclearity, which was an element of the unique RST relation between spans N and 5, is now an element, of the intentional relation and is independent of the informational relation. [0.0160426141426]
In its broadest outline, the goal is to understand the precise interaction between features of form, meaning and function in the creation of discourse coherence. [0.0155064242447]
By investigating the range of combinations of informational and intentional relations which occur with a lexical marker, we can identify the minimal description of the marker in tennis of discourse relations. [0.0153615024561]
In comparing the status of intentional and infbrinational discourse relations in both RST and GkS, at least two issues were specified whose resolution is currently presumed by both theories in isolation. [0.0152175856523]

2013-11-24 23:18:19.122106
W93-0225

2013-11-24 23:18:19.122211
Filtered Sentences

2013-11-24 23:18:19.122318
In comparing the status of intentional and infbrinational discourse relations in both RST and GkS, at least two issues were specified whose resolution is currently presumed by both theories in isolation. [0.0164547915288]
Moore and Pollack (1992) propose that, in fact, two contiguous spans of text may be in both an informational and an intentional relation simultaneously and that recognition of one kind of relation can facilitate recognition of the other. [0.0164210089013]
Informational and intentional discourse relations are essentially non-linguistic in the sense that they do not originate with language. [0.0159866211396]
These will be termed textual, informational and intentional discourse relations respectively. [0.0158386930417]
Is the informational relation between two spans of text necessarily a relation between the entire spans? [0.0156966289824]

2013-11-24 23:23:57.392027
O90-1002

2013-11-24 23:23:57.392150
All Sentences

2013-11-24 23:23:57.392304
In sentence (21), since the relation among E, R, S is E= R < S and the aspect marker is progressive, we have the &quot;past-progressive&quot; as the tense form of the translated English sentence. [0.00734672267875]
) On the contrary, in sentences linked by &quot;de5 shi2-hou4&quot; (when), &quot;yi3-hou4&quot; (after), &quot;yi3-qian2&quot; (before), the possible tense form of the event E2' can be usually determined by the relations of E, R, S. This is because in sentences of such cases, the event time of El' can be used as the reference time for the event E2'. [0.00714605747214]
(i )If the event time of El' is past (i.e. El = R2 < S), we have the relation E2 < R2 < S. Thus, the tense forms for E2' may be &quot;past-perfect&quot; or &quot;past-perfect-progressive&quot;, as the sentence (47) shows. [0.00706658689795]
They claimed that in a complex sentence, if the main clause has the tense form of&quot; present&quot;, &quot;future&quot;, or &quot;present—perfect&quot;, the tense form for the subordinate clause can be any of the twelve tense forms mentioned above. [0.00691717159132]
) (ii) If the event time of El' is future (i.e., S <E1 = R2), we have the relation S <E2 <R2 .Thus, the tense forms for E2' may be &quot;future-perfect&quot; or &quot;future-perfect-progressive&quot;. [0.00658852081147]

2013-11-24 23:24:20.518443
O90-1002

2013-11-24 23:24:20.518553
Filtered Sentences

2013-11-24 23:24:20.518656
For example, the tense form of the verb &quot;lai2&quot; (come) in sentence (45) is &quot;present&quot; while its original tense form is &quot;future&quot;. [0.00821108880404]
(i )If the event time of El' is past (i.e. El = R2 < S), we have the relation E2 < R2 < S. Thus, the tense forms for E2' may be &quot;past-perfect&quot; or &quot;past-perfect-progressive&quot;, as the sentence (47) shows. [0.00797468328014]
They claimed that in a complex sentence, if the main clause has the tense form of&quot; present&quot;, &quot;future&quot;, or &quot;present—perfect&quot;, the tense form for the subordinate clause can be any of the twelve tense forms mentioned above. [0.00789585606515]
The other two tense forms &quot;past-future&quot; and &quot;pastfuture-perfect&quot; are seldom used in a simple sentence. [0.0078539482555]
If the tense form is &quot;present-perfect&quot; and the time phrase is given specially, use &quot;past&quot; tense form instead. [0.00768625164559]

2013-11-24 23:27:44.377800
C96-1001

2013-11-24 23:27:44.377890
All Sentences

2013-11-24 23:27:44.377991
An understanding of intonational variation and the ways in which it carries information about discourse characteristics of spoken language is important for computer-based interpretation and generation of speech. [0.054038781583]
Finally, variations in intonational prominence may be used to convey information about the discourse status of entities referred to by definite noun phrases and pronouns. [0.050922766623]
It is widely accepted that discourses are composed of segments and that the recognition of segment boundaries is essential to a determination of discourse meaning (Grosz and Sidner, 1986). [0.0506423216033]
Three major challenges have faced researchers attempting to discover the relationship between intonational features and the structure of spoken discourse. [0.0502747408925]
Then I will describe results of our analyses of the correlation between discourse structure and intonational features. [0.0496992232885]

2013-11-24 23:28:29.834131
C96-1001

2013-11-24 23:28:29.834222
Filtered Sentences

2013-11-24 23:28:29.834344
Then I will describe results of our analyses of the correlation between discourse structure and intonational features. [0.0526519273522]
Three major challenges have faced researchers attempting to discover the relationship between intonational features and the structure of spoken discourse. [0.0523965929125]
In ,spoken language, intonational variation provides essential information about discourse structure. [0.050531563521]
An understanding of intonational variation and the ways in which it carries information about discourse characteristics of spoken language is important for computer-based interpretation and generation of speech. [0.0495702597087]
Because discourse structure is rooted in semantics rather than syntax, this has proved more difficult than tagging corpora for sentence structure. [0.0481776811496]
